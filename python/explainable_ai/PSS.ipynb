{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSS\n",
    "\n",
    "PSS(Parameter-Space Saliency)は，Deep Learningモデルの顕著性(Saliency)を可視化する手法の一つである．\n",
    "\n",
    "誤分類に影響したパラメータ(Weight)を分析するアプローチで，影響の大きいパラメータを補正することでDeep Learningモデルの性能を改善できることが示された．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "* 論文\n",
    "  * https://arxiv.org/abs/2108.01335\n",
    "* GitHub\n",
    "  * https://github.com/LevinRoman/parameter-space-saliency\n",
    "* 解説資料：DL輪読会\n",
    "  * https://www.slideshare.net/DeepLearningJP2016/dlwhere-do-models-go-wrong-parameterspace-saliency-maps-for-explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSS効果例\n",
    "\n",
    "論文ではGrad-CAMとの比較が示されている．  \n",
    "下図のようにPSSではGrad-CAMでは表現されない可視化要因を表現することができる(右2枚，ユキヒメドリ(junco)及び旅客列車(passenger car))．\n",
    "\n",
    "![paper figure19](./figure/PSS/paper_figure19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSS理論解説\n",
    "\n",
    "本節では[PSS論文](https://arxiv.org/abs/2108.01335)及び[GitHub](https://github.com/LevinRoman/parameter-space-saliency)のソースコードをもとに解釈した内容を記載する．\n",
    "\n",
    "論文では，パラメータ顕著性(Parameter saliency)の計算方法の説明(2.1 Parameter saliency profile)とモデルの誤動作を入力空間へ可視化する方法の説明(2.2 Input-space saliency for visualizing how filters malfunction)の2部構成で述べられる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter saliency profile\n",
    "\n",
    "パラメータ顕著性は下記の3ステップで計算する．\n",
    "\n",
    "1. パラメータ毎の顕著性の計算\n",
    "1. フィルタ毎の顕著性への集約\n",
    "1. Validationデータによる標準化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### パラメータ毎の顕著性の計算\n",
    "\n",
    "入力$x$，正解ラベル$y$のValidationデータセット$D$，及び，損失関数$\\mathcal{L}$で最小化したパラメータ$\\theta$を持つ識別モデルを仮定する．\n",
    "\n",
    "パラメータ毎の顕著性は，損失関数を識別モデルの各パラメータで偏微分して得られる勾配の大きさで定義する．  \n",
    "インデックス$i$のパラメータを$\\theta_i$で表すと，パラメータ毎の顕著性$s(x, y)_i$は以下のように定義される．\n",
    "\n",
    "$$\n",
    "  \\begin{align}\n",
    "    s(x, y)_i &:= |\\nabla_{\\theta_i}\\mathcal{L}_\\theta (x, y)|\n",
    "  \\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### フィルタ毎の顕著性への集約 \n",
    "\n",
    "畳み込みフィルタはエッジ(Edge)，形状(Shape)，質感(Texture)を検出する性質があることで知られている．\n",
    "\n",
    "顕著性$s(x, y)_i$をフィルタ毎に集約することにより，損失が最も敏感なフィルタを分離することが可能となる．つまり，分離されたフィルタを修正することによって，損失をより大きく減少させることが期待できる．\n",
    "\n",
    "識別モデルの一つのフィルタを$\\mathcal{F}_k$，フィルタ$\\mathcal{F}_k$に属するパラメータのインデックス群を$\\alpha_k$で示す．フィルタ毎の顕著性$\\bar{s}(x, y)_k$は，パラメータ毎の顕著性をフィルタ単位で平均を求めるものとして，下記のように定義される．\n",
    "\n",
    "$$\n",
    "  \\begin{align}\n",
    "    \\bar{s}(x, y)_k &:= \\frac{1}{|\\alpha_k|}\\sum_{i \\in \\alpha_k}s(x, y)_i\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "ソースコードでは下記の通り，フィルタ毎の勾配としてカーネル毎に勾配の平均を算出する．\n",
    "\n",
    "* [saliency_model_backprop.py](https://github.com/LevinRoman/parameter-space-saliency/blob/master/parameter_saliency/saliency_model_backprop.py#L49)\n",
    "```python\n",
    "for i in range(len(gradients)):  # Filter-wise aggregation\n",
    "    # print(gradients[i].size())\n",
    "\n",
    "    if self.aggregation == 'filter_wise':\n",
    "        if len(gradients[i].size()) == 4:  # If conv layer\n",
    "            if not self.signed:\n",
    "                # first take abs and then aggregate\n",
    "                filter_grads.append(gradients[i].abs().mean(-1).mean(-1).mean(-1))\n",
    "            else:\n",
    "                filter_grads.append(gradients[i].mean(-1).mean(-1).mean(-1))\n",
    "    if self.aggregation == 'parameter_wise':\n",
    "        if not self.signed:\n",
    "            filter_grads.append(gradients[i].view(-1).abs())\n",
    "        else:\n",
    "            filter_grads.append(gradients[i].view(-1))\n",
    "    if self.aggregation == 'tensor_wise':\n",
    "        raise NotImplementedError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validationデータによる標準化\n",
    "\n",
    "下図(論文Figure1)の上図は，ResNet-50で層毎の顕著性を，ImageNetのValidationデータセットに対して平均値を算出し，層毎に顕著性降順にソートしたグラフである．\n",
    "\n",
    "![paper figure1](./figure/PSS/paper_figure1.png)\n",
    "\n",
    "勾配のスケールが入力層から出力層の間で異なっていることが明らかである(入力層の顕著性が大きく，出力層に向かうにつれて小さくなる)．これにはいくつかの要因がある．\n",
    "\n",
    "1. 入力層に近いフィルタは，エッジ(Edge)や質感(Texture)等，幅広い画像に対して有効な特徴量を抽出する性質を持つ．\n",
    "  * つまり，タスクに特化したフィルタではない為，出力層のフィルタと比較した際に相対的に損失が大きくなる\n",
    "1. 一般的にネットワークを構成する際は入力層に近いほどフィルタ数が少なくなるように設計する．層あたりのフィルタ数が少ないと，各フィルタが及ぼす影響力が相対的に大きくなる．\n",
    "1. 入力層に近いフィルタの効果は，後続のネットワークへ継承される．\n",
    "  * つまり，出力層に向かうにつれて入力層側のフィルタで獲得した特徴量を破壊しないように影響度が小さくなる\n",
    "  \n",
    "そこで，スケールをフィルタ間で合わせるために，フィルタ毎にValidationデータセットで標準化する．フィルタ$k$の標準化顕著性$\\hat{s}(x, y)_k$は下記のように定義される．\n",
    "\n",
    "$$\n",
    "  \\begin{align}\n",
    "    \\hat{s}(x, y)_k &:= \\frac{|\\bar{s}(x, y)_k - \\mu_k|}{\\sigma_k}\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "これを一般化すると，\n",
    "\n",
    "$$\n",
    "  \\begin{align}\n",
    "    \\hat{s}(x, y) &:= \\frac{|\\bar{s}(x, y) - \\mu|}{\\sigma}\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "となり，畳み込みフィルタ数長のテンソルが$\\hat{s}(x, y)$として得られる．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input-space saliency for visualizing how filters malfunction\n",
    "\n",
    "上述の方法で算出した顕著性を用いて，ネットワークの誤動作や異常動作の要因となるフィルタを特定することが可能となる．\n",
    "\n",
    "具体的には，大別して下記の3ステップにより，フィルタの顕著性に影響する画像特徴量を特定することができる．\n",
    "\n",
    "1. 上位$k$個のフィルタ顕著性を選択する  \n",
    "※$k$は任意で，ソースコードでは引数で個数を指定\n",
    "1. 選択したフィルタ顕著性を定数倍(Boost)して$s'$を算出する\n",
    "1. Boost前後の顕著性($s, s'$)のコサイン類似度を計算し，その勾配の絶対値を算出する($M_F$)  \n",
    "$$\n",
    "  \\begin{align}\n",
    "    M_F = |\\nabla_x D_C(s(x, y), s')|\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "算出された$M_F$がフィルタ$F$の顕著性に影響を与えるピクセルの影響度合いを示す．\n",
    "\n",
    "\n",
    "* [parameter_and_input_saliency.py](https://github.com/LevinRoman/parameter-space-saliency/blob/master/parameter_and_input_saliency.py#L132)\n",
    "\n",
    "```python\n",
    "#Errors are a fragile concept, we should not perturb too much, we will end up on the object\n",
    "for noise_iter in range(args.noise_iters):\n",
    "    perturbed_inputs = reference_inputs.detach().clone()\n",
    "    perturbed_inputs = (1-args.noise_percent)*perturbed_inputs + args.noise_percent*torch.randn_like(perturbed_inputs)\n",
    "\n",
    "    perturbed_outputs = net(perturbed_inputs)\n",
    "    _, perturbed_predicted = perturbed_outputs.max(1)\n",
    "    # print(readable_labels[int(perturbed_predicted[0])])\n",
    "\n",
    "    #Backprop to the input\n",
    "    perturbed_inputs.requires_grad_()\n",
    "    #Find the true saliency:\n",
    "    filter_saliency = filter_saliency_model(\n",
    "        perturbed_inputs, reference_targets,\n",
    "        testset_mean_abs_grad=testset_mean_stat,\n",
    "        testset_std_abs_grad=testset_std_stat).to(device)\n",
    "\n",
    "    #Find the top-k salient filters\n",
    "    if args.compare_random:\n",
    "        sorted_filters = torch.randperm(filter_saliency.size(0)).cpu().numpy()\n",
    "    else:\n",
    "        sorted_filters = torch.argsort(filter_saliency, descending=True).cpu().numpy()\n",
    "\n",
    "    #Boost them:\n",
    "    filter_saliency_boosted = filter_saliency.detach().clone()\n",
    "    filter_saliency_boosted[sorted_filters[:args.k_salient]] *= args.boost_factor\n",
    "\n",
    "    #Form matching loss and take the gradient:\n",
    "    matching_criterion = torch.nn.CosineSimilarity()\n",
    "    matching_loss = matching_criterion(filter_saliency[None, :], filter_saliency_boosted[None, :])\n",
    "    matching_loss.backward()\n",
    "\n",
    "    grads_to_save = perturbed_inputs.grad.detach().cpu()\n",
    "    grad_samples.append(grads_to_save)\n",
    "#Find averaged gradients (smoothgrad-like)\n",
    "grads_to_save = torch.stack(grad_samples).mean(0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※下記でヒートマップを生成しているが，コサイン類似度の勾配の絶対値が意味するものが不明\n",
    "\n",
    "* [parameter_and_input_saliency.py](https://github.com/LevinRoman/parameter-space-saliency/blob/0e3b3d69c6e222aee6af0264d7ce3ddc6d19744e/parameter_and_input_saliency.py#L88)\n",
    "\n",
    "```python\n",
    "    grads_to_save = (grads_to_save - np.min(grads_to_save)) / (np.max(grads_to_save) - np.min(grads_to_save))\n",
    "\n",
    "    #Superimpose gradient heatmap\n",
    "    reference_image_to_compare = inv_transform_test(reference_image[0].cpu()).permute(1, 2, 0)\n",
    "    gradients_heatmap = np.ones_like(grads_to_save) - grads_to_save\n",
    "    gradients_heatmap = cv2.GaussianBlur(gradients_heatmap, (3, 3), 0)\n",
    "\n",
    "    #Save the heatmap\n",
    "    heatmap_superimposed = show_heatmap_on_image(reference_image_to_compare.detach().cpu().numpy(), gradients_heatmap)\n",
    "    plt.imshow(heatmap_superimposed)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(save_path, 'input_saliency_heatmap_{}.png'.format(save_name)), bbox_inches='tight')\n",
    "    print('Input space saliency saved to {} \\n'.format(os.path.join(save_path, 'input_saliency_heatmap_{}.png'.format(save_name))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSS動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'parameter-space-saliency'...\n",
      "remote: Enumerating objects: 143, done.\u001b[K\n",
      "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
      "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
      "remote: Total 143 (delta 61), reused 92 (delta 24), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (143/143), 4.68 MiB | 6.71 MiB/s, done.\n",
      "Resolving deltas: 100% (61/61), done.\n",
      "Note: checking out '0e3b3d69c6e222aee6af0264d7ce3ddc6d19744e'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 0e3b3d6 fixing filter saliency\n"
     ]
    }
   ],
   "source": [
    "if (not os.path.exists(\"parameter-space-saliency\")):\n",
    "    !git clone https://github.com/LevinRoman/parameter-space-saliency\n",
    "    !cd parameter-space-saliency ; git checkout 0e3b3d69c6e222aee6af0264d7ce3ddc6d19744e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting click\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 3.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (3.3.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
      "Collecting opencv-python==4.5.1.48\n",
      "  Downloading opencv_python-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (50.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.4 MB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 93.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathy==0.4.0\n",
      "  Downloading pathy-0.4.0-py3-none-any.whl (36 kB)\n",
      "Collecting PyYAML==5.4.1\n",
      "  Downloading PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640 kB)\n",
      "\u001b[K     |████████████████████████████████| 640 kB 79.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 95.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 29.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "\u001b[K     |████████████████████████████████| 292 kB 96.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.7.1\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\",)': /packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl\u001b[0m\n",
      "  Downloading torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 776.8 MB 32 kB/s s eta 0:00:01     |████████▊                       | 213.1 MB 19.9 MB/s eta 0:00:29\n",
      "\u001b[?25hCollecting torchvision==0.8.2\n",
      "  Downloading torchvision-0.8.2-cp36-cp36m-manylinux1_x86_64.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 26.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3==1.26.3\n",
      "  Downloading urllib3-1.26.3-py2.py3-none-any.whl (137 kB)\n",
      "\u001b[K     |████████████████████████████████| 137 kB 87.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from click->-r requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 2)) (8.2.0)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 72.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pathy==0.4.0->-r requirements.txt (line 6)) (0.8)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 90.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 82.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->-r requirements.txt (line 11)) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open<4.0.0,>=2.2.0->pathy==0.4.0->-r requirements.txt (line 6)) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open<4.0.0,>=2.2.0->pathy==0.4.0->-r requirements.txt (line 6)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->smart-open<4.0.0,>=2.2.0->pathy==0.4.0->-r requirements.txt (line 6)) (2.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open<4.0.0,>=2.2.0->pathy==0.4.0->-r requirements.txt (line 6)) (4.0.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=235b5bc6cbd2c641157fd8041e4455b25b3bf1bdb3f1753b5f0ddd38044f60b0\n",
      "  Stored in directory: /root/.cache/pip/wheels/88/2a/d4/f2e9023989d4d4b3574f268657cb6cd23994665a038803f547\n",
      "Successfully built smart-open\n",
      "Installing collected packages: click, opencv-python, pytz, pandas, typer, smart-open, pathy, PyYAML, threadpoolctl, joblib, scipy, scikit-learn, seaborn, torch, torchvision, tqdm, urllib3\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.5.2.54\n",
      "    Uninstalling opencv-python-4.5.2.54:\n",
      "      Successfully uninstalled opencv-python-4.5.2.54\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.4\n",
      "    Uninstalling urllib3-1.26.4:\n",
      "      Successfully uninstalled urllib3-1.26.4\n",
      "Successfully installed PyYAML-5.4.1 click-8.0.3 joblib-1.1.0 opencv-python-4.5.1.48 pandas-1.1.5 pathy-0.4.0 pytz-2021.3 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 smart-open-3.0.0 threadpoolctl-3.1.0 torch-1.7.1 torchvision-0.8.2 tqdm-4.62.3 typer-0.4.0 urllib3-1.26.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd parameter-space-saliency ; pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "\n",
      "               ImageNet validation set path is not specified.\n",
      "               The code will only work with raw --image_path and --image_target_label specified.\n",
      "               In this scenario, --reference_id must be None.\n",
      "              \n",
      "parameter_and_input_saliency.py:241: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  readable_labels = yaml.load(readable_labels)\n",
      "==> Building model..\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████████████████████████████████| 97.8M/97.8M [00:01<00:00, 97.1MB/s]\n",
      "Total filters: 26560\n",
      "Total layers: 53\n",
      "\n",
      "\n",
      "        Using image raw_images/great_white_shark_mispred_as_killer_whale.jpeg\n",
      "        and target label 2\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "        Image target label: 2\n",
      "        Image target class name: great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\n",
      "        Image predicted label: 148\n",
      "        Image predicted class name: killer whale, killer, orca, grampus, sea wolf, Orcinus orca\n",
      "\n",
      "        \n",
      "Input space saliency saved to figures/input_space_saliency/input_saliency_heatmap_great_white_shark_mispred_as_killer_whale_resnet50.png \n",
      "\n",
      "Filter saliency saved to figures/filter_saliency_great_white_shark_mispred_as_killer_whale_resnet50.png\n"
     ]
    }
   ],
   "source": [
    "!cd parameter-space-saliency ; python parameter_and_input_saliency.py --model resnet50 --image_path raw_images/great_white_shark_mispred_as_killer_whale.jpeg --image_target_label 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_saliency_107_densenet121.png\r\n",
      "filter_saliency_107_inception_v3.png\r\n",
      "filter_saliency_107_resnet50.png\r\n",
      "filter_saliency_107_vgg19.png\r\n",
      "filter_saliency_great_white_shark_mispred_as_killer_whale_resnet50.png\r\n",
      "input_space_saliency\r\n"
     ]
    }
   ],
   "source": [
    "!ls parameter-space-saliency/figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_saliency_heatmap_107_densenet121.png\r\n",
      "input_saliency_heatmap_107_inception_v3.png\r\n",
      "input_saliency_heatmap_107_resnet50.png\r\n",
      "input_saliency_heatmap_107_vgg19.png\r\n",
      "input_saliency_heatmap_great_white_shark_mispred_as_killer_whale_resnet50.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls parameter-space-saliency/figures/input_space_saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet 121\n",
    "\n",
    "##### Filter Saliency\n",
    "\n",
    "![DenseNet Filter Saliency](parameter-space-saliency/figures/filter_saliency_107_densenet121.png)\n",
    "\n",
    "##### Input Saliency Heatmap\n",
    "\n",
    "![DenseNet Heatmap](parameter-space-saliency/figures/input_space_saliency/input_saliency_heatmap_107_densenet121.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception v3\n",
    "\n",
    "##### Filter Saliency\n",
    "\n",
    "![Inception v3 Filter Saliency](parameter-space-saliency/figures/filter_saliency_107_inception_v3.png)\n",
    "\n",
    "##### Input Saliency Heatmap\n",
    "\n",
    "![Inception V3 Heatmap](parameter-space-saliency/figures/input_space_saliency/input_saliency_heatmap_107_inception_v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50\n",
    "\n",
    "##### Filter Saliency\n",
    "\n",
    "![ResNet50 Filter Saliency](parameter-space-saliency/figures/filter_saliency_107_resnet50.png)\n",
    "\n",
    "##### Input Saliency Heatmap\n",
    "\n",
    "![ResNet50 Heatmap](parameter-space-saliency/figures/input_space_saliency/input_saliency_heatmap_107_resnet50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50\n",
    "\n",
    "##### Filter Saliency\n",
    "\n",
    "![VGG19 Filter Saliency](parameter-space-saliency/figures/filter_saliency_107_vgg19.png)\n",
    "\n",
    "##### Input Saliency Heatmap\n",
    "\n",
    "![VGG19 Heatmap](parameter-space-saliency/figures/input_space_saliency/input_saliency_heatmap_107_vgg19.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
