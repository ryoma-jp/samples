# Explainable AI

## 概要

* 機械学習モデルの解釈性を表現する手法のサンプル

## 手法一覧

|Method|Sample Code|
|:--|:--|:--|
|[LIME(local interpretable model-agnostic explanations)](https://arxiv.org/abs/1602.04938)|(T.B.D)|
|[SHAP(SHapley Additive exPlanations)](https://arxiv.org/abs/1705.07874)|(T.B.D)|
|[Anchors](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)|(T.B.D)|
|[Influence](https://arxiv.org/abs/1703.04730)|(T.B.D)|
|[Born Again Tree](https://www.stat.berkeley.edu/~breiman/BAtrees.pdf)|(T.B.D)|
|[Making Tree Ensembles Interpretable](https://arxiv.org/abs/1606.05390)|(T.B.D)|
|[Guided Backpropagation](https://arxiv.org/abs/1412.6806)|(T.B.D)|
|Epsilon LRP|(T.B.D)|
|[Integrated Grad](https://arxiv.org/abs/1703.01365)|(T.B.D)|
|[Smooth Grad](https://arxiv.org/abs/1706.03825)|(T.B.D)|
|[Deep LIFT](https://arxiv.org/abs/1704.02685)|(T.B.D)|
|[Grad-CAM](https://arxiv.org/abs/1610.02391)|(T.B.D)|
|[Eigen-CAM](https://arxiv.org/abs/2008.00299)|(T.B.D)|
|[PSS(Parameter-Space Saliency)](https://arxiv.org/abs/2108.01335)|(T.B.D)|

## 実行手順

	docker_run.shで起動後，
		http://localhost:10001
	へブラウザでアクセスし，コンソールに表示されるトークン部分をコピペしてログインする

## 参照

* [機械学習モデルの局所的な解釈（LIMEとSHAP）](https://horomary.hatenablog.com/entry/2019/09/16/000110)
* [機械学習モデルの判断根拠の説明](https://www.slideshare.net/SatoshiHara3/ss-126157179)
* [機械学習モデルの予測根拠を説明する方法](https://www.deep-percept.co.jp/blog/category02/20201020517/)
* [Interpreting and Explaining Deep Neural Networks: A Perspective on Time Series Data](http://xai.kaist.ac.kr/Tutorial/2020/)
* [Saliency Mapを使って画像を良い感じに切り抜くAIを作った](https://qiita.com/ttyszk/items/833d3753248bbc8a211f)
* [Efficient Saliency Maps for Explainable AI](https://arxiv.org/abs/1911.11293)
* [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)

